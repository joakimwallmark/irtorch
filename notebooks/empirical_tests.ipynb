{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inital setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO: NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# PyTorch model and training necessities\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import irtorch.load_dataset as load_dataset\n",
    "import irtorch.models as models\n",
    "from irtorch import IRT\n",
    "\n",
    "torch.set_printoptions(precision=7, sci_mode=False)\n",
    "device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "latent_variables = 2\n",
    "one_hot_encoded = True\n",
    "negative_latent_variable_item_relationships = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset.big_five()\n",
    "data = data[~data.isnan().any(dim=1)]\n",
    "n_cats = [int(data[:, col].max()) + 1 for col in range(data.shape[1])]\n",
    "mc_correct = None\n",
    "train_data = data[:10000, :]\n",
    "test_data = data[10000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset.swedish_national_mathematics_2018()\n",
    "data = load_dataset.swedish_national_mathematics_2019()\n",
    "n_cats = [int(data[:, col].max()) + 1 for col in range(data.shape[1])]\n",
    "mc_correct = None\n",
    "writer = SummaryWriter('runs/natmath')\n",
    "train_data = data\n",
    "test_data = None\n",
    "model_missing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset.swedish_sat_2022_binary()[:, :80]\n",
    "data, mc_correct = load_dataset.swedish_sat_2022()\n",
    "data, mc_correct = load_dataset.swedish_sat_verbal_2022()\n",
    "data, mc_correct = load_dataset.swedish_sat_quantitative_2022()\n",
    "writer = SummaryWriter('runs/swesat')\n",
    "n_cats = (torch.where(~data.isnan(), data, torch.tensor(float('-inf'))).max(dim=0).values + 1).int().tolist()\n",
    "train_data = data[:5000, :]\n",
    "test_data = data[5000:, :]\n",
    "# model_missing = True\n",
    "model_missing = False\n",
    "train_data = train_data[~train_data.isnan().any(dim=1)]\n",
    "test_data = test_data[~test_data.isnan().any(dim=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 20 items\n",
    "# train_data = train_data[:, :20]\n",
    "# test_data = test_data[:, :20]\n",
    "# n_cats = n_cats[:20]\n",
    "# mc_correct = mc_correct[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 80% of data for training, 20% for testing\n",
    "# torch.manual_seed(125)\n",
    "# total_size  = data.shape[0]\n",
    "# train_size = int(total_size * 0.8)\n",
    "# test_size = total_size - train_size\n",
    "# indices = torch.randperm(total_size)\n",
    "# train_data = data[:train_size]\n",
    "# test_data = data[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2d903d9f2d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42) # For reproducibility purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "irt_model = models.Parametric(latent_variables=latent_variables, item_categories=n_cats, model = \"GPC\")\n",
    "irt_model = models.Parametric(latent_variables=latent_variables, item_categories=n_cats, model = \"nominal\", mc_correct=mc_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate = \"items\"\n",
    "separate = \"categories\"\n",
    "# decoder_ae = models.NonparametricMonotone(\n",
    "#     latent_variables=latent_variables, \n",
    "#     item_categories=n_cats, \n",
    "#     hidden_dim=[2]*4, \n",
    "#     mc_correct=mc_correct,\n",
    "#     separate=separate,\n",
    "#     negative_latent_variable_item_relationships=negative_latent_variable_item_relationships,\n",
    "#     use_bounded_activation=False,\n",
    "# )\n",
    "model = models.NonparametricMonotoneNN(\n",
    "    latent_variables=latent_variables, \n",
    "    item_categories=n_cats, \n",
    "    hidden_dim=[3]*1, \n",
    "    model_missing=model_missing,\n",
    "    mc_correct=mc_correct,\n",
    "    separate=separate,\n",
    "    negative_latent_variable_item_relationships=negative_latent_variable_item_relationships,\n",
    "    use_bounded_activation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = IRT(\n",
    "    model = \"MMCNN\",\n",
    "    estimation_algorithm=\"AE\",\n",
    "    latent_variables=latent_variables,\n",
    "    item_categories=n_cats,\n",
    "    model_missing=model_missing,\n",
    "    mc_correct=mc_correct,\n",
    "    one_hot_encoded=one_hot_encoded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43. Average training batch loss function: 70.3357 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Stopping training after 5 learning rate updates.\n",
      "INFO: Best model found at epoch 37 with loss 70.2297.\n"
     ]
    }
   ],
   "source": [
    "ae_model.fit(\n",
    "    train_data=train_data,\n",
    "    validation_data=None,\n",
    "    batch_size=64,\n",
    "    max_epochs=500,\n",
    "    learning_rate=0.04,\n",
    "    learning_rate_update_patience=4,\n",
    "    learning_rate_updates_before_stopping=5,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ae_model.load_model(\"models/ae_model2d.pt\")\n",
    "# ae_model.save_model(\"models/ae_model_big_five.pt\")\n",
    "# ae_model.save_model(\"models/ae_1layer_mmc_2d.pt\")\n",
    "ae_model.load_model(\"models/ae_1layer_mmc_2d.pt\")\n",
    "# ae_model.save_model(\"models/ae_1layer_mmc.pt\")\n",
    "# ae_model.load_model(\"models/ae_1layer_mmc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: ML estimation of z scores.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5: Current Loss = 264158.78125 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Converged at iteration 5.\n",
      "INFO: Estimation of population z scores needed for bit score computation.\n",
      "INFO: ML estimation of z scores.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5: Current Loss = 264158.78125 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Converged at iteration 5.\n",
      "INFO: Approximating minimum bit score z from random guessing data for latent variable 1.\n",
      "INFO: ML estimation of z scores.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11: Current Loss = 1160407.625 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Converged at iteration 11.\n",
      "INFO: Approximating minimum bit score z from random guessing data for latent variable 2.\n",
      "INFO: ML estimation of z scores.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4: Current Loss = 1085439.875 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Converged at iteration 4.\n"
     ]
    }
   ],
   "source": [
    "theta = ae_model.latent_scores(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = ae_model.latent_scores(test_data, scale=\"z\", z_estimation_method=\"NN\")\n",
    "z_train = ae_model.latent_scores(train_data, scale=\"z\", z_estimation_method=\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = ae_model.model.information(z_test[:4], item=False)\n",
    "print(g1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = ae_model.model.probability_gradients(z_test[:10])\n",
    "g1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = ae_model.latent_scores(test_data, scale=\"z\", z_estimation_method=\"ML\", ml_map_device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.log_likelihood(test_data, z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = ae_model.residuals(test_data, z_test, average_per=\"none\")\n",
    "group_residuals_std, group_means_std = ae_model.evaluator.group_fit_residuals(train_data, z_train)\n",
    "group_residuals, group_means = ae_model.evaluator.group_fit_residuals(train_data, z_train, standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(group_residuals_std[:, :, 1:][~group_residuals_std[:, :, 1:].isnan()].min())\n",
    "print(group_residuals[:, :, 1:][~group_residuals[:, :, 1:].isnan()].min())\n",
    "print(group_residuals_std[:, :, 1:][~group_residuals_std[:, :, 1:].isnan()].max())\n",
    "print(group_residuals[:, :, 1:][~group_residuals[:, :, 1:].isnan()].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sns.histplot(residuals.view(-1)[0:1000].numpy(), bins=20, kde=True, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(group_residuals.view(-1)[0:1000].numpy(), bins=20, kde=True, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(group_residuals_std.view(-1)[0:1000].numpy(), bins=20, kde=True, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_residuals = group_residuals.abs().nanmean(dim=(1, 2)).numpy()\n",
    "print(abs_residuals) # average per group\n",
    "sns.scatterplot(x=range(len(abs_residuals)), y=abs_residuals, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test_taker = 3\n",
    "z_range = torch.arange(-20, 20, 0.01).reshape(-1, 1)\n",
    "z_range_data = test_data[test_taker].repeat(z_range.shape[0], 1)\n",
    "z_range_lls = ae_model.log_likelihood(z_range_data, z=z_range, reduction=\"sum\", level = \"respondent\")\n",
    "plt.plot(z_range.view(-1).numpy(), z_range_lls.numpy())\n",
    "# ae_model.latent_scores(test_data[test_taker], scale = \"z\", z_estimation_method=\"ML\", lbfgs_learning_rate = 0.1)\n",
    "# ae_model.latent_scores(test_data[test_taker], scale = \"z\", z_estimation_method=\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = ae_model.latent_scores(test_data, scale = \"z\", z_estimation_method=\"ML\", lbfgs_learning_rate = 0.5)\n",
    "z_scoresnn = ae_model.latent_scores(test_data, scale = \"z\", z_estimation_method=\"NN\")\n",
    "print(ae_model.log_likelihood(test_data, z=z_scores, reduction=\"sum\"))\n",
    "print(ae_model.log_likelihood(test_data, z=z_scoresnn, reduction=\"sum\"))\n",
    "v1 = ae_model.log_likelihood(test_data, z=z_scores, reduction=\"sum\", level = \"respondent\")\n",
    "v3 = ae_model.log_likelihood(test_data, z=z_scoresnn, reduction=\"sum\", level = \"respondent\")\n",
    "print((v1-v3 > 0).sum())\n",
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.plot_item_probabilities(1, scale=\"entropy\", bit_score_z_grid_method=\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ae_model.log_likelihood(test_data, z_estimation_method=\"ML\")/test_data.shape[0])\n",
    "print(ae_model.log_likelihood(test_data, z_estimation_method=\"NN\")/test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ae_model.performance_measures(train_data, z_estimation_method=\"NN\"))\n",
    "print(ae_model.performance_measures(train_data, z_estimation_method=\"ML\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.all(ae_model.neuralnet.decoder.negation_dim0.all_item_weights().sign() == ae_model.expected_item_score_slopes().sign()[:, 0]))\n",
    "loading_matrix = ae_model.expected_item_score_slopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer import Rotator\n",
    "fig, ax = ae_model.plot_item_latent_variable_relationships(loading_matrix)\n",
    "fig.set_size_inches(14, 8)\n",
    "rotator = Rotator(method = \"oblimin\")\n",
    "rotator = Rotator(method = \"geomin_obl\")\n",
    "rot_loadings = torch.from_numpy(rotator.fit_transform(loading_matrix)).float()\n",
    "fig, ax = ae_model.plot_item_latent_variable_relationships(rot_loadings)\n",
    "fig.set_size_inches(14, 8)\n",
    "print(torch.matmul(torch.pinverse(loading_matrix), loading_matrix)) # inverse seems okay\n",
    "rotation_matrix = torch.matmul(torch.pinverse(loading_matrix), rot_loadings) # this is the approximated rotation matrix\n",
    "torch.matmul(ae_model.neuralnet.training_z_scores[:4, :], rotation_matrix) # scores on rotated latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading_matrix[34:35, :].abs().mean(dim=0)\n",
    "print(loading_matrix[30:36, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems we can learn the relationship with categories, but not the positve relationship with items or none\n",
    "print(ae_model.neuralnet.decoder.negation_dim0.weight_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to mirt natmat19 parameters. Now we get the same results as mirt. Thus we know ML works\n",
    "ae_model.neuralnet.decoder.weight_param.data = torch.tensor([2.5189051, 1.0967695, 1.7912886, 1.0544961, 1.3236270, 0.7682886, 1.1903453, 1.1976377, 1.0403539, 1.1382457, 1.1452582, 1.6043902, 1.5123447, 1.1255488, 1.5347988, 0.8174907, 1.2528523, 1.5601218, 1.5662433, 1.4421894, 1.5873562, 1.1329438, 1.0850653, 1.8991486, 0.6878224, 2.8491980, 1.3779171, 1.3775168])\n",
    "ae_model.neuralnet.decoder.bias_param.data = torch.tensor([5.93121597, 1.11018681, 2.67522172, 0.65849740, 0.12171197, 0.63857422, -0.28117063, 2.52399358, 0.08670245, -0.26579335, 0.32917236, 0.23361287, 0.81169356, 2.06087681, 0.95423703, -0.96461046, 1.81968773, 2.30483187, -1.16145291, -0.74049082, -1.72369722, -0.33545562, -4.36504949, 0.29119501, 0.72781671, 0.64072305, 1.24324280, -1.14788551, -1.01538586, 0.36050509, 0.42816877, -0.06657620, -1.46316508, -3.31066712, -3.42984958, -5.50889103, -6.73668716, -7.40374796, 2.45505041, 3.34499460, 0.50128754, 2.15389234, 0.79753226, 0.28523457, -0.12389297, -2.67341152, -0.32989097, -0.66872235, -0.74760225, -0.46646419, -3.83190003, -7.28969035, -2.64948762, -3.77082790, -3.05549163, -5.52320448, -6.80493089, -8.49903333])\n",
    "print(ae_model.neuralnet.decoder.item_probabilities(torch.tensor([[0.5]]))[:, 9, :])\n",
    "print(ae_model.performance_measures(train_data, z_estimation_method=\"ML\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Furter improve decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_decoder = ae_model.neuralnet.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_decoder = models.Parametric(latent_variables=latent_variables, item_categories=n_cats, model = \"gpc\")\n",
    "new_decoder = models.NonparametricMonotone(\n",
    "    latent_variables=latent_variables, \n",
    "    item_categories=n_cats, \n",
    "    hidden_dim=[6, 6, 6, 6], \n",
    "    mc_correct=mc_correct,\n",
    "    negative_latent_variable_item_relationships=negative_latent_variable_item_relationships,\n",
    "    use_bounded_activation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = ae_model.latent_scores(train_data, \"z\", \"ML\")\n",
    "torch_data = torch.utils.data.TensorDataset(torch.cat((z_scores, train_data), dim=1))\n",
    "torch_loader = torch.utils.data.DataLoader(torch_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(new_decoder.parameters(), lr=0.002)\n",
    "# Reduce learning rate when loss stops decreasing (\"min\")\n",
    "# we multiply the learning rate by the factor\n",
    "# patience: We need to improvement after 3 epochs for it to trigger\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=3, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_decoder.train()\n",
    "for epoch in range(400):\n",
    "    total_loss = 0\n",
    "    for _, batch in enumerate(torch_loader):\n",
    "        optimizer.zero_grad()\n",
    "        logits = new_decoder(batch[0][:, z_scores.shape[1]-1].view(-1, 1))\n",
    "        loss = -new_decoder.log_likelihood(batch[0][:, z_scores.shape[1]:], logits) / data.shape[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"epoch: {epoch}, loss: {total_loss}\")\n",
    "\n",
    "ae_model.neuralnet.decoder = new_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ae_model.performance_measures(test_data))\n",
    "print(ae_model.performance_measures(train_data, z_estimation_method=\"NN\"))\n",
    "print(ae_model.performance_measures(train_data, z_estimation_method=\"ML\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the z - output relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.plot_latent_score_distribution(train_data, z_estimation_method=\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_z = torch.arange(-40, 40, 0.1).unsqueeze(1)\n",
    "outputs = ae_model.neuralnet.decoder(input_z)\n",
    "print(outputs.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "for item, max_cats in enumerate([max(n_cats)]*len(n_cats)):\n",
    "    plt.plot(input_z.detach().numpy(), outputs[:, item*max_cats:(item*max_cats+n_cats[item])].detach().numpy())\n",
    "    plt.title(f\"Item {item+1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ae_model.plot_item_probabilities(2, scale = \"z\", plot_group_fit=True, group_fit_groups=10, group_z_estimation_method=\"ML\")\n",
    "ae_model.plot_item_probabilities(55, scale = \"entropy\")\n",
    "# ae_model.plot_item_probabilities(55, scale = \"z\", plot_group_fit=True, group_fit_groups=10, group_z_estimation_method=\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in range(1, 20):\n",
    "    ae_model.plot_item_probabilities(item, scale = \"z\", plot_group_fit=True, group_fit_groups=10, group_z_estimation_method=\"NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating gradients etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.neuralnet.decoder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Does not seem to work...\n",
    "from captum.attr import IntegratedGradients, LayerConductance, NeuronConductance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature attribution\n",
    "input = torch.tensor([[0.2]], dtype=torch.float)\n",
    "ig = IntegratedGradients(ae_model.neuralnet.decoder)\n",
    "# Target is logit column index in output.\n",
    "attributions, delta = ig.attribute(input, target=1, return_convergence_delta=True)\n",
    "print('IG Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer attribution for single layer\n",
    "lc = LayerConductance(ae_model.neuralnet.decoder, ae_model.neuralnet.decoder.linear0_dim0)\n",
    "lc = LayerConductance(ae_model.neuralnet.decoder, ae_model.neuralnet.decoder.linear_out_dim0)\n",
    "layer_attributions = lc.attribute(input, target=6)\n",
    "layer_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a neuron index in the layer\n",
    "nc = NeuronConductance(ae_model.neuralnet.decoder, ae_model.neuralnet.decoder.linear_out_dim0)\n",
    "neuron_attributions = nc.attribute(input, target=6, neuron_selector=6)\n",
    "neuron_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature attribution using deep lift\n",
    "from captum.attr import DeepLift\n",
    "deep_lift = DeepLift(ae_model.neuralnet.decoder)\n",
    "# Compute the attribution for a specific class\n",
    "attribution = deep_lift.attribute(torch.tensor([[0.1]]), target=1)\n",
    "# The 'attribution' tensor now holds the importance of each feature in 'sample_input'\n",
    "attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os \n",
    "if os.path.exists(\"runs/parameters\"):\n",
    "    shutil.rmtree(\"runs/parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = AutoencoderIRT(\n",
    "    decoder=model,\n",
    "    one_hot_encoded=one_hot_encoded,\n",
    "    multiple_choice_correct_categories=mc_correct,\n",
    "    summary_writer=SummaryWriter('runs/parameters')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.fit(\n",
    "    train_data=train_data,\n",
    "    validation_data=test_data,\n",
    "    batch_size=32,\n",
    "    epochs=8,\n",
    "    learning_rate=0.02,\n",
    "    device=device,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "layer = ae_model.neuralnet.decoder.linear_out_dim0\n",
    "weights = layer.raw_weight_param\n",
    "weights = F.softmax(weights)\n",
    "weights\n",
    "weights.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor both CPU and GPU and record tensor shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_training\"):\n",
    "        ae_model.fit(\n",
    "            train_data=train_data,\n",
    "            validation_data=test_data,\n",
    "            batch_size=32,\n",
    "            epochs=3,\n",
    "            learning_rate=0.02,\n",
    "            device=device,\n",
    "            verbose=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_result = prof.key_averages(group_by_input_shape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof_result.table(sort_by=\"cuda_time_total\", row_limit=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_vae = models.NonparametricMonotone(latent_variables=1, item_categories=n_cats, hidden_dim=[6], use_bounded_activation=True)\n",
    "decoder_vae = models.Parametric(item_categories=n_cats, latent_variables=latent_variables, model = \"gpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = VariationalAutoencoderIRT(\n",
    "        decoder=decoder_vae,\n",
    "        one_hot_encoded=one_hot_encoded,\n",
    "        multiple_choice_correct_categories=mc_correct\n",
    "    )\n",
    "vae_model.fit(\n",
    "    train_data=data,\n",
    "    batch_size=32,\n",
    "    epochs=60,\n",
    "    learning_rate=0.002,\n",
    "    device=device,\n",
    "    annealing_epochs=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vae_model.performance_measures(test_data))\n",
    "print(vae_model.performance_measures(train_data, z_estimation_method=\"NN\"))\n",
    "print(vae_model.performance_measures(train_data, z_estimation_method=\"ML\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the model\n",
    "add_graph() will trace the sample input through your model,\n",
    "and render it as a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(ae_model.neuralnet, train_data)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(vae_model.neuralnet, test_data)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you switch over to TensorBoard, you should see a GRAPHS tab. Double-click the nodes to see the layers and data flow within the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
