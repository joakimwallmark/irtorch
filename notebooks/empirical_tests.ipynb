{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inital setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO: NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# PyTorch model and training necessities\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import irtorch.load_dataset as load_dataset\n",
    "import irtorch.models as models\n",
    "from irtorch import IRT\n",
    "\n",
    "torch.set_printoptions(precision=7, sci_mode=False)\n",
    "device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "latent_variables = 2\n",
    "one_hot_encoded = True\n",
    "negative_latent_variable_item_relationships = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset.big_five()\n",
    "data = data[~data.isnan().any(dim=1)]\n",
    "n_cats = [int(data[:, col].max()) + 1 for col in range(data.shape[1])]\n",
    "mc_correct = None\n",
    "train_data = data[:10000, :]\n",
    "test_data = data[10000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset.swedish_national_mathematics_2018()\n",
    "data = load_dataset.swedish_national_mathematics_2019()\n",
    "n_cats = [int(data[:, col].max()) + 1 for col in range(data.shape[1])]\n",
    "mc_correct = None\n",
    "writer = SummaryWriter('runs/natmath')\n",
    "train_data = data\n",
    "test_data = None\n",
    "model_missing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset.swedish_sat_2022_binary()[:, :80]\n",
    "data, mc_correct = load_dataset.swedish_sat_2022()\n",
    "data, mc_correct = load_dataset.swedish_sat_verbal_2022()\n",
    "data, mc_correct = load_dataset.swedish_sat_quantitative_2022()\n",
    "writer = SummaryWriter('runs/swesat')\n",
    "n_cats = (torch.where(~data.isnan(), data, torch.tensor(float('-inf'))).max(dim=0).values + 1).int().tolist()\n",
    "train_data = data[:5000, :]\n",
    "test_data = data[5000:, :]\n",
    "# model_missing = True\n",
    "model_missing = False\n",
    "train_data = train_data[~train_data.isnan().any(dim=1)]\n",
    "test_data = test_data[~test_data.isnan().any(dim=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 20 items\n",
    "# train_data = train_data[:, :20]\n",
    "# test_data = test_data[:, :20]\n",
    "# n_cats = n_cats[:20]\n",
    "# mc_correct = mc_correct[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 80% of data for training, 20% for testing\n",
    "# torch.manual_seed(125)\n",
    "# total_size  = data.shape[0]\n",
    "# train_size = int(total_size * 0.8)\n",
    "# test_size = total_size - train_size\n",
    "# indices = torch.randperm(total_size)\n",
    "# train_data = data[:train_size]\n",
    "# test_data = data[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1be341832d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42) # For reproducibility purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "irt_model = models.Parametric(latent_variables=latent_variables, item_categories=n_cats, model = \"GPC\")\n",
    "irt_model = models.Parametric(latent_variables=latent_variables, item_categories=n_cats, model = \"nominal\", mc_correct=mc_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate = \"items\"\n",
    "separate = \"categories\"\n",
    "# decoder_ae = models.NonparametricMonotone(\n",
    "#     latent_variables=latent_variables, \n",
    "#     item_categories=n_cats, \n",
    "#     hidden_dim=[2]*4, \n",
    "#     mc_correct=mc_correct,\n",
    "#     separate=separate,\n",
    "#     negative_latent_variable_item_relationships=negative_latent_variable_item_relationships,\n",
    "#     use_bounded_activation=False,\n",
    "# )\n",
    "model = models.NonparametricMonotoneNN(\n",
    "    latent_variables=latent_variables, \n",
    "    item_categories=n_cats, \n",
    "    hidden_dim=[3]*1, \n",
    "    model_missing=model_missing,\n",
    "    mc_correct=mc_correct,\n",
    "    separate=separate,\n",
    "    negative_latent_variable_item_relationships=negative_latent_variable_item_relationships,\n",
    "    use_bounded_activation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = IRT(\n",
    "    model = \"MMCNN\",\n",
    "    estimation_algorithm=\"AE\",\n",
    "    latent_variables=latent_variables,\n",
    "    item_categories=n_cats,\n",
    "    model_missing=model_missing,\n",
    "    mc_correct=mc_correct,\n",
    "    one_hot_encoded=one_hot_encoded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ae_model.fit(\n",
    "#     train_data=train_data,\n",
    "#     validation_data=None,\n",
    "#     batch_size=64,\n",
    "#     max_epochs=500,\n",
    "#     learning_rate=0.04,\n",
    "#     learning_rate_update_patience=4,\n",
    "#     learning_rate_updates_before_stopping=5,\n",
    "#     device=device,\n",
    "#     verbose=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ae_model.load_model(\"models/ae_model2d.pt\")\n",
    "# ae_model.save_model(\"models/ae_model_big_five.pt\")\n",
    "# ae_model.save_model(\"models/ae_1layer_mmc_2d.pt\")\n",
    "ae_model.load_model(\"models/ae_1layer_mmc_2d.pt\")\n",
    "# ae_model.save_model(\"models/ae_1layer_mmc.pt\")\n",
    "# ae_model.load_model(\"models/ae_1layer_mmc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML iteration 1: Loss = 264165.65625\n",
      "ML iteration 2: Loss = 264159.5625\n",
      "ML iteration 3: Loss = 264159.3125\n",
      "ML iteration 4: Loss = 264158.5\n",
      "ML iteration 5: Loss = 264158.375\n",
      "ML iteration 6: Loss = 264158.125\n",
      "ML iteration 7: Loss = 264158.09375\n",
      "ML iteration 8: Loss = 264157.875\n",
      "ML iteration 9: Loss = 264157.875\n",
      "Converged at iteration 9\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m theta \u001b[38;5;241m=\u001b[39m ae_model\u001b[38;5;241m.\u001b[39mlatent_scores(train_data)\n",
      "File \u001b[1;32mc:\\users\\wallm\\git\\irtorch\\irtorch\\irt.py:223\u001b[0m, in \u001b[0;36mIRT.latent_scores\u001b[1;34m(self, data, scale, z, z_estimation_method, ml_map_device, lbfgs_learning_rate, eap_z_integration_points, entropy_one_dimensional, entropy_grid_points, entropy_z_grid_method, entropy_start_z, entropy_start_z_guessing_probabilities, entropy_start_z_guessing_iterations, entropy_items)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlatent_scores\u001b[39m(\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    168\u001b[0m     data: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m     entropy_items: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    182\u001b[0m ):\n\u001b[0;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    Returns the latent scores for given test data using encoder the neural network (NN), maximum likelihood (ML), expected a posteriori (EAP) or maximum a posteriori (MAP). \u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m    ML and MAP uses the LBFGS algorithm. EAP and MAP are not recommended for non-variational autoencoder models as there is nothing pulling the latent distribution towards a normal.        \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m        A 2D tensor of latent scores, with latent variables as columns.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mlatent_scores(\n\u001b[0;32m    224\u001b[0m         data,\n\u001b[0;32m    225\u001b[0m         scale,\n\u001b[0;32m    226\u001b[0m         z,\n\u001b[0;32m    227\u001b[0m         z_estimation_method,\n\u001b[0;32m    228\u001b[0m         ml_map_device,\n\u001b[0;32m    229\u001b[0m         lbfgs_learning_rate,\n\u001b[0;32m    230\u001b[0m         eap_z_integration_points,\n\u001b[0;32m    231\u001b[0m         entropy_one_dimensional,\n\u001b[0;32m    232\u001b[0m         entropy_grid_points,\n\u001b[0;32m    233\u001b[0m         entropy_z_grid_method,\n\u001b[0;32m    234\u001b[0m         entropy_start_z,\n\u001b[0;32m    235\u001b[0m         entropy_start_z_guessing_probabilities,\n\u001b[0;32m    236\u001b[0m         entropy_start_z_guessing_iterations,\n\u001b[0;32m    237\u001b[0m         entropy_items,\n\u001b[0;32m    238\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\wallm\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\wallm\\git\\irtorch\\irtorch\\irt_scorer.py:189\u001b[0m, in \u001b[0;36mIRTScorer.latent_scores\u001b[1;34m(self, data, scale, z, z_estimation_method, ml_map_device, lbfgs_learning_rate, eap_z_integration_points, entropy_one_dimensional, entropy_population_z, entropy_grid_points, entropy_z_grid_method, entropy_start_z, entropy_start_z_guessing_probabilities, entropy_start_z_guessing_iterations, entropy_items)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entropy_z_grid_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     entropy_z_grid_method \u001b[38;5;241m=\u001b[39m z_estimation_method\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_entropy_scores_from_z(\n\u001b[0;32m    190\u001b[0m     z\u001b[38;5;241m=\u001b[39mz,\n\u001b[0;32m    191\u001b[0m     start_z\u001b[38;5;241m=\u001b[39mentropy_start_z,\n\u001b[0;32m    192\u001b[0m     population_z\u001b[38;5;241m=\u001b[39mentropy_population_z,\n\u001b[0;32m    193\u001b[0m     one_dimensional\u001b[38;5;241m=\u001b[39mentropy_one_dimensional,\n\u001b[0;32m    194\u001b[0m     z_estimation_method\u001b[38;5;241m=\u001b[39mentropy_z_grid_method,\n\u001b[0;32m    195\u001b[0m     grid_points\u001b[38;5;241m=\u001b[39mentropy_grid_points,\n\u001b[0;32m    196\u001b[0m     items\u001b[38;5;241m=\u001b[39mentropy_items,\n\u001b[0;32m    197\u001b[0m     start_z_guessing_probabilities\u001b[38;5;241m=\u001b[39mentropy_start_z_guessing_probabilities,\n\u001b[0;32m    198\u001b[0m     start_z_guessing_iterations\u001b[38;5;241m=\u001b[39mentropy_start_z_guessing_iterations\n\u001b[0;32m    199\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\wallm\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\wallm\\git\\irtorch\\irtorch\\irt_scorer.py:548\u001b[0m, in \u001b[0;36mIRTScorer._entropy_scores_from_z\u001b[1;34m(self, z, start_z, population_z, one_dimensional, z_estimation_method, ml_map_device, lbfgs_learning_rate, grid_points, items, start_z_guessing_probabilities, start_z_guessing_iterations)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode()\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_entropy_scores_from_z\u001b[39m(\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     start_z_guessing_iterations: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m,\n\u001b[0;32m    514\u001b[0m ):\n\u001b[0;32m    515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;124;03m    Computes the entropy scores from z scores.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m        A 2D tensor with entropy distance scale scores for each respondent across the rows together with another tensor with start_z.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grid_points \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    549\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps must be a positive integer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_z \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m start_z\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlatent_variables):\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "theta = ae_model.latent_scores(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = ae_model.latent_scores(test_data, scale=\"z\", z_estimation_method=\"NN\")\n",
    "z_train = ae_model.latent_scores(train_data, scale=\"z\", z_estimation_method=\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 2])\n",
      "tensor([[0.2633193, 0.0028491],\n",
      "        [0.0028491, 0.4074665]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "g1 = ae_model.model.information(z_test[:4], item=False)\n",
    "print(g1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 80, 5, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = ae_model.model.probability_gradients(z_test[:10])\n",
    "g1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = ae_model.latent_scores(test_data, scale=\"z\", z_estimation_method=\"ML\", ml_map_device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.log_likelihood(test_data, z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = ae_model.residuals(test_data, z_test, average_per=\"none\")\n",
    "group_residuals_std, group_means_std = ae_model.evaluator.group_fit_residuals(train_data, z_train)\n",
    "group_residuals, group_means = ae_model.evaluator.group_fit_residuals(train_data, z_train, standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(group_residuals_std[:, :, 1:][~group_residuals_std[:, :, 1:].isnan()].min())\n",
    "print(group_residuals[:, :, 1:][~group_residuals[:, :, 1:].isnan()].min())\n",
    "print(group_residuals_std[:, :, 1:][~group_residuals_std[:, :, 1:].isnan()].max())\n",
    "print(group_residuals[:, :, 1:][~group_residuals[:, :, 1:].isnan()].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sns.histplot(residuals.view(-1)[0:1000].numpy(), bins=20, kde=True, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(group_residuals.view(-1)[0:1000].numpy(), bins=20, kde=True, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(group_residuals_std.view(-1)[0:1000].numpy(), bins=20, kde=True, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_residuals = group_residuals.abs().nanmean(dim=(1, 2)).numpy()\n",
    "print(abs_residuals) # average per group\n",
    "sns.scatterplot(x=range(len(abs_residuals)), y=abs_residuals, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test_taker = 3\n",
    "z_range = torch.arange(-20, 20, 0.01).reshape(-1, 1)\n",
    "z_range_data = test_data[test_taker].repeat(z_range.shape[0], 1)\n",
    "z_range_lls = ae_model.log_likelihood(z_range_data, z=z_range, reduction=\"sum\", level = \"respondent\")\n",
    "plt.plot(z_range.view(-1).numpy(), z_range_lls.numpy())\n",
    "# ae_model.latent_scores(test_data[test_taker], scale = \"z\", z_estimation_method=\"ML\", lbfgs_learning_rate = 0.1)\n",
    "# ae_model.latent_scores(test_data[test_taker], scale = \"z\", z_estimation_method=\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = ae_model.latent_scores(test_data, scale = \"z\", z_estimation_method=\"ML\", lbfgs_learning_rate = 0.5)\n",
    "z_scoresnn = ae_model.latent_scores(test_data, scale = \"z\", z_estimation_method=\"NN\")\n",
    "print(ae_model.log_likelihood(test_data, z=z_scores, reduction=\"sum\"))\n",
    "print(ae_model.log_likelihood(test_data, z=z_scoresnn, reduction=\"sum\"))\n",
    "v1 = ae_model.log_likelihood(test_data, z=z_scores, reduction=\"sum\", level = \"respondent\")\n",
    "v3 = ae_model.log_likelihood(test_data, z=z_scoresnn, reduction=\"sum\", level = \"respondent\")\n",
    "print((v1-v3 > 0).sum())\n",
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.plot_item_probabilities(1, scale=\"entropy\", entropy_z_grid_method=\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ae_model.log_likelihood(test_data, z_estimation_method=\"ML\")/test_data.shape[0])\n",
    "print(ae_model.log_likelihood(test_data, z_estimation_method=\"NN\")/test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ae_model.performance_measures(train_data, z_estimation_method=\"NN\"))\n",
    "print(ae_model.performance_measures(train_data, z_estimation_method=\"ML\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.all(ae_model.neuralnet.decoder.negation_dim0.all_item_weights().sign() == ae_model.expected_item_score_slopes().sign()[:, 0]))\n",
    "loading_matrix = ae_model.expected_item_score_slopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer import Rotator\n",
    "fig, ax = ae_model.plot_item_latent_variable_relationships(loading_matrix)\n",
    "fig.set_size_inches(14, 8)\n",
    "rotator = Rotator(method = \"oblimin\")\n",
    "rotator = Rotator(method = \"geomin_obl\")\n",
    "rot_loadings = torch.from_numpy(rotator.fit_transform(loading_matrix)).float()\n",
    "fig, ax = ae_model.plot_item_latent_variable_relationships(rot_loadings)\n",
    "fig.set_size_inches(14, 8)\n",
    "print(torch.matmul(torch.pinverse(loading_matrix), loading_matrix)) # inverse seems okay\n",
    "rotation_matrix = torch.matmul(torch.pinverse(loading_matrix), rot_loadings) # this is the approximated rotation matrix\n",
    "torch.matmul(ae_model.neuralnet.training_z_scores[:4, :], rotation_matrix) # scores on rotated latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading_matrix[34:35, :].abs().mean(dim=0)\n",
    "print(loading_matrix[30:36, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems we can learn the relationship with categories, but not the positve relationship with items or none\n",
    "print(ae_model.neuralnet.decoder.negation_dim0.weight_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to mirt natmat19 parameters. Now we get the same results as mirt. Thus we know ML works\n",
    "ae_model.neuralnet.decoder.weight_param.data = torch.tensor([2.5189051, 1.0967695, 1.7912886, 1.0544961, 1.3236270, 0.7682886, 1.1903453, 1.1976377, 1.0403539, 1.1382457, 1.1452582, 1.6043902, 1.5123447, 1.1255488, 1.5347988, 0.8174907, 1.2528523, 1.5601218, 1.5662433, 1.4421894, 1.5873562, 1.1329438, 1.0850653, 1.8991486, 0.6878224, 2.8491980, 1.3779171, 1.3775168])\n",
    "ae_model.neuralnet.decoder.bias_param.data = torch.tensor([5.93121597, 1.11018681, 2.67522172, 0.65849740, 0.12171197, 0.63857422, -0.28117063, 2.52399358, 0.08670245, -0.26579335, 0.32917236, 0.23361287, 0.81169356, 2.06087681, 0.95423703, -0.96461046, 1.81968773, 2.30483187, -1.16145291, -0.74049082, -1.72369722, -0.33545562, -4.36504949, 0.29119501, 0.72781671, 0.64072305, 1.24324280, -1.14788551, -1.01538586, 0.36050509, 0.42816877, -0.06657620, -1.46316508, -3.31066712, -3.42984958, -5.50889103, -6.73668716, -7.40374796, 2.45505041, 3.34499460, 0.50128754, 2.15389234, 0.79753226, 0.28523457, -0.12389297, -2.67341152, -0.32989097, -0.66872235, -0.74760225, -0.46646419, -3.83190003, -7.28969035, -2.64948762, -3.77082790, -3.05549163, -5.52320448, -6.80493089, -8.49903333])\n",
    "print(ae_model.neuralnet.decoder.item_probabilities(torch.tensor([[0.5]]))[:, 9, :])\n",
    "print(ae_model.performance_measures(train_data, z_estimation_method=\"ML\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Furter improve decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_decoder = ae_model.neuralnet.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_decoder = models.Parametric(latent_variables=latent_variables, item_categories=n_cats, model = \"gpc\")\n",
    "new_decoder = models.NonparametricMonotone(\n",
    "    latent_variables=latent_variables, \n",
    "    item_categories=n_cats, \n",
    "    hidden_dim=[6, 6, 6, 6], \n",
    "    mc_correct=mc_correct,\n",
    "    negative_latent_variable_item_relationships=negative_latent_variable_item_relationships,\n",
    "    use_bounded_activation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = ae_model.latent_scores(train_data, \"z\", \"ML\")\n",
    "torch_data = torch.utils.data.TensorDataset(torch.cat((z_scores, train_data), dim=1))\n",
    "torch_loader = torch.utils.data.DataLoader(torch_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(new_decoder.parameters(), lr=0.002)\n",
    "# Reduce learning rate when loss stops decreasing (\"min\")\n",
    "# we multiply the learning rate by the factor\n",
    "# patience: We need to improvement after 3 epochs for it to trigger\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=3, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_decoder.train()\n",
    "for epoch in range(400):\n",
    "    total_loss = 0\n",
    "    for _, batch in enumerate(torch_loader):\n",
    "        optimizer.zero_grad()\n",
    "        logits = new_decoder(batch[0][:, z_scores.shape[1]-1].view(-1, 1))\n",
    "        loss = -new_decoder.log_likelihood(batch[0][:, z_scores.shape[1]:], logits) / data.shape[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"epoch: {epoch}, loss: {total_loss}\")\n",
    "\n",
    "ae_model.neuralnet.decoder = new_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ae_model.performance_measures(test_data))\n",
    "print(ae_model.performance_measures(train_data, z_estimation_method=\"NN\"))\n",
    "print(ae_model.performance_measures(train_data, z_estimation_method=\"ML\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the z - output relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.plot_latent_score_distribution(train_data, z_estimation_method=\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_z = torch.arange(-40, 40, 0.1).unsqueeze(1)\n",
    "outputs = ae_model.neuralnet.decoder(input_z)\n",
    "print(outputs.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "for item, max_cats in enumerate([max(n_cats)]*len(n_cats)):\n",
    "    plt.plot(input_z.detach().numpy(), outputs[:, item*max_cats:(item*max_cats+n_cats[item])].detach().numpy())\n",
    "    plt.title(f\"Item {item+1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ae_model.plot_item_probabilities(2, scale = \"z\", plot_group_fit=True, group_fit_groups=10, group_z_estimation_method=\"ML\")\n",
    "ae_model.plot_item_probabilities(55, scale = \"entropy\")\n",
    "# ae_model.plot_item_probabilities(55, scale = \"z\", plot_group_fit=True, group_fit_groups=10, group_z_estimation_method=\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in range(1, 20):\n",
    "    ae_model.plot_item_probabilities(item, scale = \"z\", plot_group_fit=True, group_fit_groups=10, group_z_estimation_method=\"NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating gradients etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.neuralnet.decoder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Does not seem to work...\n",
    "from captum.attr import IntegratedGradients, LayerConductance, NeuronConductance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature attribution\n",
    "input = torch.tensor([[0.2]], dtype=torch.float)\n",
    "ig = IntegratedGradients(ae_model.neuralnet.decoder)\n",
    "# Target is logit column index in output.\n",
    "attributions, delta = ig.attribute(input, target=1, return_convergence_delta=True)\n",
    "print('IG Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer attribution for single layer\n",
    "lc = LayerConductance(ae_model.neuralnet.decoder, ae_model.neuralnet.decoder.linear0_dim0)\n",
    "lc = LayerConductance(ae_model.neuralnet.decoder, ae_model.neuralnet.decoder.linear_out_dim0)\n",
    "layer_attributions = lc.attribute(input, target=6)\n",
    "layer_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a neuron index in the layer\n",
    "nc = NeuronConductance(ae_model.neuralnet.decoder, ae_model.neuralnet.decoder.linear_out_dim0)\n",
    "neuron_attributions = nc.attribute(input, target=6, neuron_selector=6)\n",
    "neuron_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature attribution using deep lift\n",
    "from captum.attr import DeepLift\n",
    "deep_lift = DeepLift(ae_model.neuralnet.decoder)\n",
    "# Compute the attribution for a specific class\n",
    "attribution = deep_lift.attribute(torch.tensor([[0.1]]), target=1)\n",
    "# The 'attribution' tensor now holds the importance of each feature in 'sample_input'\n",
    "attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os \n",
    "if os.path.exists(\"runs/parameters\"):\n",
    "    shutil.rmtree(\"runs/parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = AutoencoderIRT(\n",
    "    decoder=model,\n",
    "    one_hot_encoded=one_hot_encoded,\n",
    "    multiple_choice_correct_categories=mc_correct,\n",
    "    summary_writer=SummaryWriter('runs/parameters')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.fit(\n",
    "    train_data=train_data,\n",
    "    validation_data=test_data,\n",
    "    batch_size=32,\n",
    "    epochs=8,\n",
    "    learning_rate=0.02,\n",
    "    device=device,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "layer = ae_model.neuralnet.decoder.linear_out_dim0\n",
    "weights = layer.raw_weight_param\n",
    "weights = F.softmax(weights)\n",
    "weights\n",
    "weights.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor both CPU and GPU and record tensor shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_training\"):\n",
    "        ae_model.fit(\n",
    "            train_data=train_data,\n",
    "            validation_data=test_data,\n",
    "            batch_size=32,\n",
    "            epochs=3,\n",
    "            learning_rate=0.02,\n",
    "            device=device,\n",
    "            verbose=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_result = prof.key_averages(group_by_input_shape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof_result.table(sort_by=\"cuda_time_total\", row_limit=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_vae = models.NonparametricMonotone(latent_variables=1, item_categories=n_cats, hidden_dim=[6], use_bounded_activation=True)\n",
    "decoder_vae = models.Parametric(item_categories=n_cats, latent_variables=latent_variables, model = \"gpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = VariationalAutoencoderIRT(\n",
    "        decoder=decoder_vae,\n",
    "        one_hot_encoded=one_hot_encoded,\n",
    "        multiple_choice_correct_categories=mc_correct\n",
    "    )\n",
    "vae_model.fit(\n",
    "    train_data=data,\n",
    "    batch_size=32,\n",
    "    epochs=60,\n",
    "    learning_rate=0.002,\n",
    "    device=device,\n",
    "    annealing_epochs=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vae_model.performance_measures(test_data))\n",
    "print(vae_model.performance_measures(train_data, z_estimation_method=\"NN\"))\n",
    "print(vae_model.performance_measures(train_data, z_estimation_method=\"ML\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the model\n",
    "add_graph() will trace the sample input through your model,\n",
    "and render it as a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(ae_model.neuralnet, train_data)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(vae_model.neuralnet, test_data)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you switch over to TensorBoard, you should see a GRAPHS tab. Double-click the nodes to see the layers and data flow within the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
